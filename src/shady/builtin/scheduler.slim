@Builtin type TreeNode = struct {
    mask_t threads;
    u32 depth;
};

@Builtin type JoinPoint = struct {
    TreeNode node;
    u32 destination;
};

@Builtin subgroup u32 actual_subgroup_size;

@Builtin subgroup u32 scheduler_cursor = 0;
@Builtin subgroup [TreeNode; SUBGROUP_SIZE] scheduler_vector;
@Builtin subgroup [u32; SUBGROUP_SIZE] resume_at;

@Builtin subgroup u32 next_fn;
@Builtin subgroup TreeNode active_branch;

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("setup_stack_frames") @Leaf
fn builtin_init_scheduler() {
    val init_mask = subgroup_active_mask();

    val tree_node1 = composite TreeNode(init_mask, 1);
    scheduler_vector#(subgroup_local_id()) = tree_node1;
    active_branch = tree_node1;

    actual_subgroup_size = subgroup_reduce_sum(u32 1);
}

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("setup_stack_frames") @Leaf
fn builtin_entry_join_point uniform JoinPoint() {
    val init_mask = subgroup_active_mask();

    val tree_node0 = composite TreeNode(init_mask, 0);
    val jp = composite JoinPoint(tree_node0, 0);
    return (jp);
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn builtin_create_control_point uniform JoinPoint(uniform u32 join_destination) {
    val curr_mask = subgroup_active_mask();
    val depth = subgroup_broadcast_first(scheduler_vector#(subgroup_local_id())#1);
    val tree_node = composite TreeNode(curr_mask, depth);
    val jp = composite JoinPoint(tree_node, join_destination);

    // increase the depth of the active leaves
    scheduler_vector#(subgroup_local_id())#1 = scheduler_vector#(subgroup_local_id())#1 + u32 1;

    return (jp);
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn builtin_fork(varying u32 branch_destination) {
    val first_branch = subgroup_broadcast_first(branch_destination);

    // if there is disagreement on the destination, then increase the depth of every branch
    val uniform_branch = subgroup_active_mask() == subgroup_ballot(first_branch == branch_destination);
    if (!uniform_branch) {
        // update depth counter
        val old_depth = scheduler_vector#(subgroup_local_id())#1;
        scheduler_vector#(subgroup_local_id())#1 = old_depth + u32 1;
    }

    // Partition the set of branch destinations and adapt the masks in turn
    loop() {
        val elected = subgroup_broadcast_first(branch_destination);
        if (elected == branch_destination) {
            resume_at#(subgroup_local_id()) = elected;
            scheduler_vector#(subgroup_local_id())#0 = subgroup_ballot(elected == branch_destination);
            break;
        }
    }

    // We must pick one branch as our 'favourite child' to schedule for immediate execution#
    // we could do fancy intrinsics, but for now we'll just pick the first one
    if (subgroup_elect_first()) {
        next_fn = subgroup_broadcast_first(branch_destination);
        active_branch = subgroup_broadcast_first(scheduler_vector#(subgroup_local_id()));

        // tag those variables as not in use#
        // resume_at#(subgroup_local_id()) = -1;
        // resume_with#(subgroup_local_id()) = empty_mask();
        // return ();
    }
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn builtin_yield(uniform u32 resume_target) {
    resume_at#(subgroup_local_id()) = resume_target;
    // resume_with#(subgroup_local_id()) = subgroup_active_mask();

    // only one thread runs that part
    if (subgroup_elect_first()) {
        // bump the cursor
        // TODO bump it in a smarter way
        scheduler_cursor = (scheduler_cursor + u32 1) % SUBGROUP_SIZE;
        leaf_call(builtin_find_schedulable_leaf)();
    }
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn builtin_join(uniform u32 join_at, uniform TreeNode token) {
    resume_at#(subgroup_local_id()) = join_at;
    scheduler_vector#(subgroup_local_id()) = token;

    // only one thread runs that part
    if (subgroup_elect_first()) {
        leaf_call(builtin_find_schedulable_leaf)();
    }
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn is_parent bool(varying TreeNode child, varying TreeNode maybe_parent) {
  val child_mask = child#0;
  val parent_mask = maybe_parent#0;
  if ((child_mask | parent_mask) != parent_mask) { return(false); }
  val child_depth = child#1;
  val parent_depth = maybe_parent#1;
  return (child_depth >= parent_depth);
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn forward_distance u32(varying u32 x, varying u32 dst, varying u32 max_mod) {
  var u32 t = dst - x;
  t = t % max_mod;
  return (t);
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn reduce2 u32(varying u32 a_index, varying u32 b_index) {
    val a = scheduler_vector#a_index;
    val b = scheduler_vector#b_index;
    
    if (leaf_call(is_parent)(a, b)) { return (a_index); }
    if (leaf_call(is_parent)(b, a)) { return (b_index); }
    
    val a_dist = leaf_call(forward_distance)(a_index, scheduler_cursor, actual_subgroup_size);
    val b_dist = leaf_call(forward_distance)(b_index, scheduler_cursor, actual_subgroup_size);
    
    if (a_dist < b_dist) { return (a_index); }
    return (b_index);
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn builtin_find_schedulable_leaf() {
    var u32 reduced = u32 0;
    loop (uniform u32 i = u32 1) {
        if (i >= actual_subgroup_size) { break; }
        reduced = leaf_call(reduce2)(reduced, i);
        continue(i + u32 1);
    }

    next_fn = subgroup_broadcast_first(resume_at#reduced);
    active_branch = subgroup_broadcast_first(scheduler_vector#reduced);
    return ();
}

@Builtin @DisablePass("lower_cf_instrs") @Leaf
fn builtin_get_active_branch mask_t() {
    val this_thread_branch = scheduler_vector#(subgroup_local_id());
    val same_dest = resume_at#(subgroup_local_id()) == next_fn;
    val not_escaping = leaf_call(is_parent)(this_thread_branch, active_branch);
    return (subgroup_ballot(same_dest & not_escaping));
}
